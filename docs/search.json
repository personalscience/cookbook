[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Science Cookbook",
    "section": "",
    "text": "We’ll assume some basic tools.\n\nA spreadsheet like Microsoft Excel\nThe programming language R and the associated development environment RStudio\n\nA good introduction to R is Hands On Programming with R\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\n\n\n\n\nThis is book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\).\nRemember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #.\nTo compile this example to PDF, you need XeLaTeX. We recommend TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "In other words, like a hungry person in a kitchen full of ingredients, you need a cookbook of recipes that can explain in a step-by-step, repeatable manner, how to go from the raw data around you to some fully-baked insights. That’s the purpose of the Personal Science Cookbook. Each “recipe” is short and self-contained. Some are more complex than others, but none require any tools or knowledge beyond what is explained in the book."
  },
  {
    "objectID": "02-literature.html",
    "href": "02-literature.html",
    "title": "3  The Principles of Personal Science",
    "section": "",
    "text": "When the first microchips enabled desktop computers in the 1970s, people were unsure what to call them.\nThe word “mini-computer” was already taken, referring to a generation of computers that didn’t require entire rooms, so the techie engineers who confronted these new machines called them “microcomputers”, a moniker that lives on in the name for one of the first software companies of that generation, Microsoft.\nSome people called them “hobby computers”, because that seemed to be all they were good for. The most influential early gathering of people using them was called the “Homebrew Computer Club”. The term “desktop” was gaining traction, and inspired later generations that called them “laptops”, but then the most traditional of all computer companies introduced its first “IBM PC”, and suddenly the industry had a new term.\nIt was a “personal computer” because, for the first time, it was cheap enough and easy enough for a single individual to use it by him (or her) self. In contrast to all previous generations of computing, everything about the device was intended to be used by a single individual. Even if the computer was shared, only one person would use it at a time, and all design decisions reflected that: a single keyboard, monitor, one power switch. You didn’t need a team of people to set up and care for the device — it was out-of-the-box something that a single person could set up and use.\nIt’s easy to forget how transformative this was at the time. Computers until then were very expensive — many times more than the cost of a car or even a house. You had to be a large organization — a university, a business — to afford one, and even if somebody magically just gave one to you, you’d need a special place to keep it, with highly-trained technicians just to keep it running, and of course even more well-trained engineers and scientists to get it to do anything useful.\n\nA similar situation exists today in science. New discoveries are made in large institutions, by teams of high-trained people with access to large, expensive equipment. The discoveries are discussed and shared by specialists who are followed by a cadre of specialized interpreters — journalists, educators, clinicians —  who decipher the new scientific results into lay language and ultimately into face-to-face interaction with the public. Committees meet to discuss takeaways from the expensive and time-consuming research, reaching conclusions that are considered generally acceptable enough to result in new actionable treatments and suggestions for “normal people”.\nThis gap between the specialists and the general public, like the gap between mainframe computers and PCs, is eroding thanks to technology.\nActually that’s not quite true: the potential gap between specialists and the general public is eroding. But reality is still different. It’s as if PCs had been invented but no software.\nThe personal computer revolution was about more than simply cheaper devices. The hardware became useful after it spawned an entire industry of dedicated software makers,  educational experts, consultants and systems integrators,\nProfessional science\nWe all think science is great…\nbut what do people mean when they say “science”? 1. Wonder  (photos of stars, micrographs, etc.) 2. Technology (photos of roman arch, integrated circuit, moon landing) 3. A way of thinking (photos of “amateur” scientists)\nIt’s tempting to assume that the scientific way of thinking is obvious, and maybe even obviously the only way to think rigorously but that’s not really true.\nAlternatives to the scientific way of thinking: recipes\nMy definition of science: a predisposition to the assumption that you’re wrong, a nasty mischievous inclination to disbelieve things you can’t prove.\nA core scientific skill is curiosity.  Always ask “what if…”  thinking in hypotheticals\nReligion seems like a classic example of unscientific thinking, but even that I’ll challenge. What if you’re wrong?  Is there a way to experiment, test it?\nScience is:\n\nCuriosity\nSkepticism : an unending belief that you are wrong\n\nLow interest in credentials … just because you are “certified” doesn’t mean you know any more than I do.\n\nBias toward experiments\n\nSee Roberts (2004) for examples.\n\n\n\n\nRoberts, Seth. 2004. “Self-Experimentation as a Source of New Ideas: Ten Examples about Sleep, Mood, Health, and Weight.” http://www.escholarship.org/uc/item/2xc2h866."
  },
  {
    "objectID": "03-method.html",
    "href": "03-method.html",
    "title": "4  Methods",
    "section": "",
    "text": "Self-collected data is almost always best represented by a table of the variables you want to study and the values that you collected for each of those variables. The most common type of table is a spreadsheet, a specific form of which in Personal Science we refer to as a data table or a data frame. Abbreviated “dataframe” or often just “df”, it’s a table of values and variables that always has the same form:\n\ncolumns are variables: the parameters you want to study\nrows are observations: each incident of data you collected.\n\nIt’s important to get in the habit of this row/column approach to data collection because, as you’ll see, all of our tools assume that data will come in a dataframe format."
  },
  {
    "objectID": "03-method.html#is-it-chance-t-test",
    "href": "03-method.html#is-it-chance-t-test",
    "title": "4  Methods",
    "section": "4.2 Is it chance? T-Test",
    "text": "4.2 Is it chance? T-Test\nProblem\nYou tried an intervention and want to see if it worked. How likely is it that the results were chance?\nSolution\nOne of the simplest tests is a “T-Test”, sometimes called a “Student T Test”.\nStatisticians use the concept of P Value to discuss the how often a result might appear to be significant even when it’s not. While this crude measure doesn’t describe all the ways something might happen due to chance, generally the lower the P Value, the better. Professional scientists, especially those who understand statistics, will get touchy if you claim a result based purely on P Values, but for Personal Science purposes, it’s a good start. There is no “correct” cutoff value that can determine the likelihood that something is due to chance alone, but traditionally people assume that anything under 0.05 deserves a closer look.\nHere’s an example for how to do this in Excel.\nSuppose you’d like to know if taking a melotonin supplement will help you sleep longer. You’ve measured your daily sleep, taking the supplements on some days (the “intervention”) and not on others (“control”).\nA simple spreadsheet might look like this:\n\nTrack your sleep under two columns: one for nights when you took the supplement, and the other for nights you didn’t.\nThe built-in Excel statistical function T.TEST will calculate the P-Value when you give it two ranges, the “intervention” (nights we took melotonin) and the “control” (nights without).\nSee the screenshot for the exact formula in this case:\n=T.TEST(array1,array2,tails,type)\nEnter a 1 for tails (because we’re only interested in one measurement, sleep) and a 2 for type (because in this case our samples are not of the same length).\nThe P Value in this example, 0.24, is above 0.05 and therefore we will assume that any difference in sleep between the nights is due to pure chance."
  },
  {
    "objectID": "03-method.html#rolling-average",
    "href": "03-method.html#rolling-average",
    "title": "4  Methods",
    "section": "4.3 Rolling average",
    "text": "4.3 Rolling average\nProblem You want to take the rolling 7-day average of a series of numbers.\n\nlibrary(tidyverse)\nheadache_df <- readr::read_csv(\"headache-variables.csv\")\nheadache_df %>% head() %>% knitr::kable()\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\n\n\n\n\n2022-06-03\nFALSE\nFALSE\n7.923619\n0\n\n\n2022-06-04\nFALSE\nFALSE\n7.733691\n0\n\n\n2022-06-05\nFALSE\nFALSE\n7.502736\n0\n\n\n2022-06-06\nFALSE\nTRUE\n5.673220\n0\n\n\n2022-06-07\nFALSE\nFALSE\n8.219025\n0\n\n\n2022-06-08\nFALSE\nFALSE\n6.172853\n0\n\n\n\n\n\nSolution use the rolling() functions in package zoo:\n\nlibrary(zoo)\n\nheadache_df %>% \n    mutate(sleep7A = rollapply(z,\n                               7, \n                               function(x) {x = mean(x,na.rm = TRUE)},\n                               align = 'right',\n                               fill = NA)) %>% \n  tail() %>% knitr::kable()\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\nsleep7A\n\n\n\n\n2022-09-03\nFALSE\nFALSE\n6.760909\n0\n6.841356\n\n\n2022-09-04\nFALSE\nFALSE\n6.026144\n0\n6.742033\n\n\n2022-09-05\nFALSE\nFALSE\n7.441416\n0\n6.913072\n\n\n2022-09-06\nFALSE\nFALSE\n5.995251\n0\n6.754675\n\n\n2022-09-07\nFALSE\nFALSE\n5.630758\n0\n6.435244\n\n\n2022-09-08\nFALSE\nFALSE\n5.677058\n0\n6.321036"
  },
  {
    "objectID": "04-application.html",
    "href": "04-application.html",
    "title": "5  Applications",
    "section": "",
    "text": "Some significant applications are demonstrated in this chapter.\nLet’s say you are suffering from unexplained headaches that appear somewhat randomly. You suspect they may be associated with something you eat, but you’re not sure, so you’ve been tracking 14 weeks (98 days) worth of your own data in a spreadsheet that looks like this:\nYou can download a copy of this file here\nWith my 14 weeks of data, we can do a few basic calculations:\nHow frequent are my headaches? Simply total the number of headaches and divide by number of days:"
  },
  {
    "objectID": "04-application.html#hypothesis",
    "href": "04-application.html#hypothesis",
    "title": "5  Applications",
    "section": "5.1 Hypothesis",
    "text": "5.1 Hypothesis\nWith the data collected and in a nice dataframe format, we can start to ask what might be driving the headaches. One of the first suspected culpritsmight be something that I eat. It’s easy to add a few more variables (columns) to the dataframe: (download)\n\nz <- function(x){\nm = NULL\nfor(i in 1:14){\n  m = c(c(rep(0,6),\n              floor(runif(1,min=0,max=3))),\n        m)\n}\n\nm\n}\nx <- tibble(date=seq(from = today()-weeks(14),\n                     by = \"1 day\", length.out = 7*14),\n            headache = sample(c(TRUE,FALSE), 7*14,\n                              prob = c(.05,.95),\n                              replace = TRUE),\n            icecream = sample(c(TRUE,FALSE), 7*14,\n                              prob = c(.10,.90),\n                              replace = TRUE),\n            z = runif(7*14, min = -2.5, max = .5) + 8,\n            wine = z(0))\n\nknitr::kable( head(x,10), digits = 2) %>%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    date \n    headache \n    icecream \n    z \n    wine \n  \n \n\n  \n    2022-06-03 \n    TRUE \n    FALSE \n    7.61 \n    0 \n  \n  \n    2022-06-04 \n    FALSE \n    FALSE \n    5.66 \n    0 \n  \n  \n    2022-06-05 \n    FALSE \n    FALSE \n    7.03 \n    0 \n  \n  \n    2022-06-06 \n    FALSE \n    TRUE \n    7.63 \n    0 \n  \n  \n    2022-06-07 \n    FALSE \n    FALSE \n    6.27 \n    0 \n  \n  \n    2022-06-08 \n    FALSE \n    FALSE \n    7.78 \n    0 \n  \n  \n    2022-06-09 \n    FALSE \n    FALSE \n    5.97 \n    2 \n  \n  \n    2022-06-10 \n    FALSE \n    FALSE \n    6.55 \n    0 \n  \n  \n    2022-06-11 \n    FALSE \n    FALSE \n    7.98 \n    0 \n  \n  \n    2022-06-12 \n    FALSE \n    FALSE \n    7.15 \n    0 \n  \n\n\n\n\nwrite_csv(x,\"headache-variables.csv\")\n\n\nheadache: a day when I have a headache\nicecream: did I eat ice cream that day?\nwine: Number of glasses of wine I drank.\nz: Number of hours I slept that day.\n\nBased on the data collected so far, can I make any guesses about what might be driving my headaches?\nThe most obvious place to check is whether I see any patterns on the days when I have headaches. Let’s filter for headache days only:\n\nx %>% filter(headache)  %>% kableExtra::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    date \n    headache \n    icecream \n    z \n    wine \n  \n \n\n  \n    2022-06-03 \n    TRUE \n    FALSE \n    7.611354 \n    0 \n  \n  \n    2022-07-09 \n    TRUE \n    FALSE \n    5.852930 \n    0 \n  \n  \n    2022-07-21 \n    TRUE \n    FALSE \n    8.331022 \n    1 \n  \n  \n    2022-08-16 \n    TRUE \n    FALSE \n    5.550140 \n    0 \n  \n\n\n\n\n\nBut maybe the headache takes a day or two to kick in. We can divide the data by week and see if we can spot any patterns in headache frequency:\n\nx %>% group_by(week = ntile(date,7)) %>% \n  summarise(headaches = sum(headache),\n            alcohol = sum(wine),\n            icecream = sum(icecream))  %>% kableExtra::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n \n  \n    week \n    headaches \n    alcohol \n    icecream \n  \n \n\n  \n    1 \n    1 \n    3 \n    1 \n  \n  \n    2 \n    0 \n    3 \n    1 \n  \n  \n    3 \n    1 \n    3 \n    2 \n  \n  \n    4 \n    1 \n    3 \n    2 \n  \n  \n    5 \n    0 \n    2 \n    0 \n  \n  \n    6 \n    1 \n    2 \n    1 \n  \n  \n    7 \n    0 \n    2 \n    0 \n  \n\n\n\n\n\nBuy simply eye-balling the data this way, you might see a pattern. For example, you might spot a week or two with an unusually large number of headaches and notice those weeks are accompanied by an unusually large consumption of some particular food.\nBut how do you know you’re not just guessing? What looks like a pattern might be a coincidence. To find out with more certainty, we will apply some statistics."
  },
  {
    "objectID": "04-application.html#t-testing",
    "href": "04-application.html#t-testing",
    "title": "5  Applications",
    "section": "5.2 T-Testing",
    "text": "5.2 T-Testing\nThe simplest test is called a “T Test”. This is a formula that can compare two equal-sized lists of numbers and return the probability that any differences between the two are the result of chance.\nWhat are the chances that the number of headaches per week is related to the amount of ice cream I eat per week?\nIf there were a relationship between ice cream and headaches each week, I’d expect that over the weeks in this period, the total number of headaches and the total number of ice cream days should be roughly equal.\n\nx_week <- x %>% group_by(week = ntile(date,7)) %>% \n  summarise(headaches = sum(headache),\n            alcohol = sum(wine),\n            icecream = sum(icecream)) \nx_week %>% kableExtra::kable() %>%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) \n\n\n\n \n  \n    week \n    headaches \n    alcohol \n    icecream \n  \n \n\n  \n    1 \n    1 \n    3 \n    1 \n  \n  \n    2 \n    0 \n    3 \n    1 \n  \n  \n    3 \n    1 \n    3 \n    2 \n  \n  \n    4 \n    1 \n    3 \n    2 \n  \n  \n    5 \n    0 \n    2 \n    0 \n  \n  \n    6 \n    1 \n    2 \n    1 \n  \n  \n    7 \n    0 \n    2 \n    0 \n  \n\n\n\n\nwith(x_week, t.test(headaches,icecream))[[\"p.value\"]]\n\n[1] 0.2713838\n\n\nBy convention, a p-value less than 0.5 is considered statistically significant. While this is not a hard and fast rule, it’s often a good place to start. A p-value greater than this is almost certainly due to chance."
  },
  {
    "objectID": "04-application.html#data-visualization",
    "href": "04-application.html#data-visualization",
    "title": "5  Applications",
    "section": "5.3 Data visualization",
    "text": "5.3 Data visualization\nThe first step in a more sophisticated analysis is to plot the data to see if we can spot any particular patterns.\n\nx_week %>% pivot_longer(names_to = \"activity\",\n                        values_to = \"amount\",\n                        cols = alcohol:icecream ) %>% \n  ggplot(aes(x=week,y=headaches)) +\n  geom_bar(aes(x=week,y=amount, fill = activity),\n           position = \"dodge\",\n           stat = \"identity\") +\n  geom_line(aes(x=week,y=headaches))"
  },
  {
    "objectID": "04-application.html#example-two",
    "href": "04-application.html#example-two",
    "title": "5  Applications",
    "section": "5.4 Example two",
    "text": "5.4 Example two"
  },
  {
    "objectID": "05-summary.html",
    "href": "05-summary.html",
    "title": "6  Final Words",
    "section": "",
    "text": "We have finished a nice book."
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "7  References",
    "section": "",
    "text": "Roberts, Seth. 2004. “Self-Experimentation as a Source of New\nIdeas: Ten Examples about Sleep, Mood, Health, and\nWeight.” http://www.escholarship.org/uc/item/2xc2h866."
  },
  {
    "objectID": "Excel - ttest.html",
    "href": "Excel - ttest.html",
    "title": "5  Is it chance? T-Test",
    "section": "",
    "text": "You tried an intervention and want to see if it worked. How likely is it that the results were chance?\nSolution\nOne of the simplest tests is a “T-Test”, sometimes called a “Student T Test”.\nStatisticians use the concept of P Value to discuss the how often a result might appear to be significant even when it’s not. While this crude measure doesn’t describe all the ways something might happen due to chance, generally the lower the P Value, the better. Professional scientists, especially those who understand statistics, will get touchy if you claim a result based purely on P Values, but for Personal Science purposes, it’s a good start. There is no “correct” cutoff value that can determine the likelihood that something is due to chance alone, but traditionally people assume that anything under 0.05 deserves a closer look.\nHere’s an example for how to do this in Excel.\nSuppose you’d like to know if taking a melotonin supplement will help you sleep longer. You’ve measured your daily sleep, taking the supplements on some days (the “intervention”) and not on others (“control”).\nA simple spreadsheet might look like this:\n\nTrack your sleep under two columns: one for nights when you took the supplement, and the other for nights you didn’t.\nThe built-in Excel statistical function T.TEST will calculate the P-Value when you give it two ranges, the “intervention” (nights we took melotonin) and the “control” (nights without).\nSee the screenshot for the exact formula in this case:\n=T.TEST(array1,array2,tails,type)\nEnter a 1 for tails (because we’re only interested in one measurement, sleep) and a 2 for type (because in this case our samples are not of the same length).\nThe P Value in this example, 0.24, is above 0.05 and therefore we will assume that any difference in sleep between the nights is due to pure chance."
  },
  {
    "objectID": "method.html",
    "href": "method.html",
    "title": "4  Methods",
    "section": "",
    "text": "Self-collected data is almost always best represented by a table of the variables you want to study and the values that you collected for each of those variables. The most common type of table is a spreadsheet, a specific form of which in Personal Science we refer to as a data table or a data frame. Abbreviated “dataframe” or often just “df”, it’s a table of values and variables that always has the same form:\n\ncolumns are variables: the parameters you want to study\nrows are observations: each incident of data you collected.\n\nIt’s important to get in the habit of this row/column approach to data collection because, as you’ll see, all of our tools assume that data will come in a dataframe format."
  },
  {
    "objectID": "method.html#is-it-chance-t-test",
    "href": "method.html#is-it-chance-t-test",
    "title": "4  Methods",
    "section": "4.2 Is it chance? T-Test",
    "text": "4.2 Is it chance? T-Test\nProblem\nYou tried an intervention and want to see if it worked. How likely is it that the results were chance?\nSolution\nOne of the simplest tests is a “T-Test”, sometimes called a “Student T Test”.\nStatisticians use the concept of P Value to discuss the how often a result might appear to be significant even when it’s not. While this crude measure doesn’t describe all the ways something might happen due to chance, generally the lower the P Value, the better. Professional scientists, especially those who understand statistics, will get touchy if you claim a result based purely on P Values, but for Personal Science purposes, it’s a good start. There is no “correct” cutoff value that can determine the likelihood that something is due to chance alone, but traditionally people assume that anything under 0.05 deserves a closer look.\nHere’s an example for how to do this in Excel.\nSuppose you’d like to know if taking a melotonin supplement will help you sleep longer. You’ve measured your daily sleep, taking the supplements on some days (the “intervention”) and not on others (“control”).\nA simple spreadsheet might look like this:\n\nTrack your sleep under two columns: one for nights when you took the supplement, and the other for nights you didn’t.\nThe built-in Excel statistical function T.TEST will calculate the P-Value when you give it two ranges, the “intervention” (nights we took melotonin) and the “control” (nights without).\nSee the screenshot for the exact formula in this case:\n=T.TEST(array1,array2,tails,type)\nEnter a 1 for tails (because we’re only interested in one measurement, sleep) and a 2 for type (because in this case our samples are not of the same length).\nThe P Value in this example, 0.24, is above 0.05 and therefore we will assume that any difference in sleep between the nights is due to pure chance."
  },
  {
    "objectID": "method.html#rolling-average",
    "href": "method.html#rolling-average",
    "title": "4  Methods",
    "section": "4.3 Rolling average",
    "text": "4.3 Rolling average\nProblem You want to take the rolling 7-day average of a series of numbers.\n\nlibrary(tidyverse)\nheadache_df <- readr::read_csv(\"headache-variables.csv\")\nheadache_df %>% head() %>% knitr::kable()\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\n\n\n\n\n2022-06-03\nFALSE\nFALSE\n5.814952\n0\n\n\n2022-06-04\nFALSE\nFALSE\n5.635999\n0\n\n\n2022-06-05\nFALSE\nFALSE\n7.805655\n0\n\n\n2022-06-06\nFALSE\nFALSE\n5.958808\n0\n\n\n2022-06-07\nFALSE\nFALSE\n8.275710\n0\n\n\n2022-06-08\nFALSE\nFALSE\n8.325172\n0\n\n\n\n\n\nSolution use the rolling() functions in package zoo:\n\nlibrary(zoo)\n\nheadache_df %>% \n    mutate(sleep7A = rollapply(z,\n                               7, \n                               function(x) {x = mean(x,na.rm = TRUE)},\n                               align = 'right',\n                               fill = NA)) %>% \n  tail() %>% knitr::kable()\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\nsleep7A\n\n\n\n\n2022-09-03\nFALSE\nFALSE\n6.606695\n0\n7.330490\n\n\n2022-09-04\nFALSE\nTRUE\n6.992533\n0\n7.138071\n\n\n2022-09-05\nFALSE\nFALSE\n7.829593\n0\n7.097343\n\n\n2022-09-06\nFALSE\nFALSE\n7.048118\n0\n7.077938\n\n\n2022-09-07\nTRUE\nFALSE\n6.889078\n0\n7.169150\n\n\n2022-09-08\nFALSE\nFALSE\n5.790681\n0\n6.935629"
  }
]