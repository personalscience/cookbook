[["index.html", "Personal Science Cookbook Chapter 1 Prerequisites 1.1 Assumptions 1.2 How to edit this book", " Personal Science Cookbook Friends of Personal Science 2022-08-12 Chapter 1 Prerequisites 1.1 Assumptions We’ll assume some basic tools. A spreadsheet like Microsoft Excel The programming language R and the associated development environment RStudio A good introduction to R is Hands On Programming with R library(tidyverse) library(lubridate) library(kableExtra) 1.2 How to edit this book This is book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. We recommend TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction There’s something you want to understand, probably about yourself, maybe something health- or wellness-related, but it might be something about the world around you. The point is that it’s a question of deep interest to you, though unlikely in its current form to be of enough interest to involve professionals. You’d like to apply the principles of science — hypothesis, experiment, analysis — but you don’t know enough of the mechanics to get started. In other words, like a hungry person in a kitchen full of ingredients, you need a cookbook of recipes that can explain in a step-by-step, repeatable manner, how to go from the raw data around you to some fully-baked insights. That’s the purpose of the Personal Science Cookbook. Each “recipe” is short and self-contained. Some are more complex than others, but none require any tools or knowledge beyond what is explained in the book. "],["the-principles-of-personal-science.html", "Chapter 3 The Principles of Personal Science", " Chapter 3 The Principles of Personal Science Personal Science is about empowering normal people to use the tools of science to help themselves in their daily lives. When the first microchips enabled desktop computers in the 1970s, people were unsure what to call them. The word “mini-computer” was already taken, referring to a generation of computers that didn’t require entire rooms, so the techie engineers who confronted these new machines called them “microcomputers”, a moniker that lives on in the name for one of the first software companies of that generation, Microsoft. Some people called them “hobby computers”, because that seemed to be all they were good for. The most influential early gathering of people using them was called the “Homebrew Computer Club”. The term “desktop” was gaining traction, and inspired later generations that called them “laptops”, but then the most traditional of all computer companies introduced its first “IBM PC”, and suddenly the industry had a new term. It was a “personal computer” because, for the first time, it was cheap enough and easy enough for a single individual to use it by him (or her) self. In contrast to all previous generations of computing, everything about the device was intended to be used by a single individual. Even if the computer was shared, only one person would use it at a time, and all design decisions reflected that: a single keyboard, monitor, one power switch. You didn’t need a team of people to set up and care for the device — it was out-of-the-box something that a single person could set up and use. It’s easy to forget how transformative this was at the time. Computers until then were very expensive — many times more than the cost of a car or even a house. You had to be a large organization — a university, a business — to afford one, and even if somebody magically just gave one to you, you’d need a special place to keep it, with highly-trained technicians just to keep it running, and of course even more well-trained engineers and scientists to get it to do anything useful. A similar situation exists today in science. New discoveries are made in large institutions, by teams of high-trained people with access to large, expensive equipment. The discoveries are discussed and shared by specialists who are followed by a cadre of specialized interpreters — journalists, educators, clinicians —  who decipher the new scientific results into lay language and ultimately into face-to-face interaction with the public. Committees meet to discuss takeaways from the expensive and time-consuming research, reaching conclusions that are considered generally acceptable enough to result in new actionable treatments and suggestions for “normal people”. This gap between the specialists and the general public, like the gap between mainframe computers and PCs, is eroding thanks to technology. Actually that’s not quite true: the potential gap between specialists and the general public is eroding. But reality is still different. It’s as if PCs had been invented but no software. The personal computer revolution was about more than simply cheaper devices. The hardware became useful after it spawned an entire industry of dedicated software makers,  educational experts, consultants and systems integrators, Professional science We all think science is great… but what do people mean when they say “science”? 1. Wonder  (photos of stars, micrographs, etc.) 2. Technology (photos of roman arch, integrated circuit, moon landing) 3. A way of thinking (photos of “amateur” scientists) It’s tempting to assume that the scientific way of thinking is obvious, and maybe even obviously the only way to think rigorously but that’s not really true. Alternatives to the scientific way of thinking: recipes My definition of science: a predisposition to the assumption that you’re wrong, a nasty mischievous inclination to disbelieve things you can’t prove. A core scientific skill is curiosity.  Always ask “what if…”  thinking in hypotheticals Religion seems like a classic example of unscientific thinking, but even that I’ll challenge. What if you’re wrong?  Is there a way to experiment, test it? Science is: Curiosity Skepticism : an unending belief that you are wrong Low interest in credentials … just because you are “certified” doesn’t mean you know any more than I do. Bias toward experiments See Roberts (2004) for examples. References "],["methods.html", "Chapter 4 Methods 4.1 What is a dataframe? 4.2 Is it chance? T-Test 4.3 Rolling average", " Chapter 4 Methods 4.1 What is a dataframe? Self-collected data is almost always best represented by a table of the variables you want to study and the values that you collected for each of those variables. The most common type of table is a spreadsheet, a specific form of which in Personal Science we refer to as a data table or a data frame. Abbreviated “dataframe” or often just “df”, it’s a table of values and variables that always has the same form: columns are variables: the parameters you want to study rows are observations: each incident of data you collected. It’s important to get in the habit of this row/column approach to data collection because, as you’ll see, all of our tools assume that data will come in a dataframe format. 4.2 Is it chance? T-Test Problem You tried an intervention and want to see if it worked. How likely is it that the results were chance? Solution One of the simplest tests is a “T-Test”, sometimes called a “Student T Test”. Statisticians use the concept of P Value to discuss the how often a result might appear to be significant even when it’s not. While this crude measure doesn’t describe all the ways something might happen due to chance, generally the lower the P Value, the better. Professional scientists, especially those who understand statistics, will get touchy if you claim a result based purely on P Values, but for Personal Science purposes, it’s a good start. There is no “correct” cutoff value that can determine the likelihood that something is due to chance alone, but traditionally people assume that anything under 0.05 deserves a closer look. Here’s an example for how to do this in Excel. Suppose you’d like to know if taking a melotonin supplement will help you sleep longer. You’ve measured your daily sleep, taking the supplements on some days (the “intervention”) and not on others (“control”). A simple spreadsheet might look like this: Track your sleep under two columns: one for nights when you took the supplement, and the other for nights you didn’t. The built-in Excel statistical function T.TEST will calculate the P-Value when you give it two ranges, the “intervention” (nights we took melotonin) and the “control” (nights without). See the screenshot for the exact formula in this case: =T.TEST(array1,array2,tails,type) Enter a 1 for tails (because we’re only interested in one measurement, sleep) and a 2 for type (because in this case our samples are not of the same length). The P Value in this example, 0.24, is above 0.05 and therefore we will assume that any difference in sleep between the nights is due to pure chance. 4.3 Rolling average Problem You want to take the rolling 7-day average of a series of numbers. headache_df &lt;- read_csv(&quot;headache-variables.csv&quot;) headache_df %&gt;% head() %&gt;% knitr::kable() date headache icecream z wine 2022-05-06 FALSE FALSE 5.766158 0 2022-05-07 FALSE FALSE 7.761379 0 2022-05-08 FALSE FALSE 8.498130 0 2022-05-09 FALSE TRUE 6.473249 0 2022-05-10 FALSE FALSE 8.135594 0 2022-05-11 FALSE FALSE 6.498094 0 Solution use the rolling() functions in package zoo: library(zoo) ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric headache_df %&gt;% mutate(sleep7A = rollapply(z, 7, function(x) {x = mean(x,na.rm = TRUE)}, align = &#39;right&#39;, fill = NA)) %&gt;% tail() %&gt;% knitr::kable() date headache icecream z wine sleep7A 2022-08-06 FALSE FALSE 8.394102 0 7.404312 2022-08-07 FALSE FALSE 8.333627 0 7.604312 2022-08-08 FALSE FALSE 7.179132 0 7.461996 2022-08-09 FALSE TRUE 5.643082 0 7.173676 2022-08-10 FALSE FALSE 8.184409 0 7.306650 2022-08-11 FALSE FALSE 6.329536 0 7.385212 "],["applications.html", "Chapter 5 Applications 5.1 Hypothesis 5.2 T-Testing 5.3 Data visualization 5.4 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. Let’s say you are suffering from unexplained headaches that appear somewhat randomly. You suspect they may be associated with something you eat, but you’re not sure, so you’ve been tracking 14 weeks (98 days) worth of your own data in a spreadsheet that looks like this: x &lt;- tibble(date=seq(from = today()-weeks(14), by = &quot;1 day&quot;, length.out = 7*14), headache = sample(c(TRUE,FALSE), 7*14, prob = c(.05,.95), replace = TRUE)) knitr::kable( head(x) ) %&gt;% kableExtra::kable_styling() date headache 2022-05-06 FALSE 2022-05-07 FALSE 2022-05-08 FALSE 2022-05-09 FALSE 2022-05-10 FALSE 2022-05-11 FALSE write_csv(x,&quot;headache-days.csv&quot;) You can download a copy of this file here With my 14 weeks of data, we can do a few basic calculations: How frequent are my headaches? Simply total the number of headaches and divide by number of days: # headaches per day sum(x$headache) / length(x$headache) ## [1] 0.03061224 5.1 Hypothesis With the data collected and in a nice dataframe format, we can start to ask what might be driving the headaches. One of the first suspected culpritsmight be something that I eat. It’s easy to add a few more variables (columns) to the dataframe: (download) z &lt;- function(x){ m = NULL for(i in 1:14){ m = c(c(rep(0,6), floor(runif(1,min=0,max=3))), m) } m } x &lt;- tibble(date=seq(from = today()-weeks(14), by = &quot;1 day&quot;, length.out = 7*14), headache = sample(c(TRUE,FALSE), 7*14, prob = c(.05,.95), replace = TRUE), icecream = sample(c(TRUE,FALSE), 7*14, prob = c(.10,.90), replace = TRUE), z = runif(7*14, min = -2.5, max = .5) + 8, wine = z(0)) knitr::kable( head(x,10), digits = 2) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) date headache icecream z wine 2022-05-06 FALSE FALSE 8.16 0 2022-05-07 FALSE FALSE 6.99 0 2022-05-08 FALSE FALSE 5.51 0 2022-05-09 FALSE FALSE 7.57 0 2022-05-10 FALSE FALSE 7.73 0 2022-05-11 FALSE FALSE 7.52 0 2022-05-12 FALSE FALSE 5.54 1 2022-05-13 FALSE FALSE 6.61 0 2022-05-14 FALSE FALSE 8.09 0 2022-05-15 FALSE FALSE 8.16 0 write_csv(x,&quot;headache-variables.csv&quot;) headache: a day when I have a headache icecream: did I eat ice cream that day? wine: Number of glasses of wine I drank. z: Number of hours I slept that day. Based on the data collected so far, can I make any guesses about what might be driving my headaches? The most obvious place to check is whether I see any patterns on the days when I have headaches. Let’s filter for headache days only: x %&gt;% filter(headache) %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) date headache icecream z wine 2022-05-26 TRUE FALSE 7.673426 0 2022-06-28 TRUE FALSE 6.630578 0 2022-07-05 TRUE FALSE 8.249089 0 2022-07-16 TRUE FALSE 6.214911 0 2022-08-06 TRUE FALSE 5.648103 0 2022-08-09 TRUE FALSE 6.623467 0 But maybe the headache takes a day or two to kick in. We can divide the data by week and see if we can spot any patterns in headache frequency: x %&gt;% group_by(week = ntile(date,7)) %&gt;% summarise(headaches = sum(headache), alcohol = sum(wine), icecream = sum(icecream)) %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) week headaches alcohol icecream 1 0 1 1 2 1 1 1 3 0 3 1 4 1 2 1 5 1 2 2 6 1 1 1 7 2 4 1 Buy simply eye-balling the data this way, you might see a pattern. For example, you might spot a week or two with an unusually large number of headaches and notice those weeks are accompanied by an unusually large consumption of some particular food. But how do you know you’re not just guessing? What looks like a pattern might be a coincidence. To find out with more certainty, we will apply some statistics. 5.2 T-Testing The simplest test is called a “T Test”. This is a formula that can compare two equal-sized lists of numbers and return the probability that any differences between the two are the result of chance. What are the chances that the number of headaches per week is related to the amount of ice cream I eat per week? If there were a relationship between ice cream and headaches each week, I’d expect that over the weeks in this period, the total number of headaches and the total number of ice cream days should be roughly equal. x_week &lt;- x %&gt;% group_by(week = ntile(date,7)) %&gt;% summarise(headaches = sum(headache), alcohol = sum(wine), icecream = sum(icecream)) x_week %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;)) week headaches alcohol icecream 1 0 1 1 2 1 1 1 3 0 3 1 4 1 2 1 5 1 2 2 6 1 1 1 7 2 4 1 with(x_week, t.test(headaches,icecream))[[&quot;p.value&quot;]] ## [1] 0.3609739 By convention, a p-value less than 0.5 is considered statistically significant. While this is not a hard and fast rule, it’s often a good place to start. A p-value greater than this is almost certainly due to chance. 5.3 Data visualization The first step in a more sophisticated analysis is to plot the data to see if we can spot any particular patterns. x_week %&gt;% pivot_longer(names_to = &quot;activity&quot;, values_to = &quot;amount&quot;, cols = alcohol:icecream ) %&gt;% ggplot(aes(x=week,y=headaches)) + geom_bar(aes(x=week,y=amount, fill = activity), position = &quot;dodge&quot;, stat = &quot;identity&quot;) + geom_line(aes(x=week,y=headaches)) 5.4 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
