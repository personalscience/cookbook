[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Science Cookbook",
    "section": "",
    "text": "This book is intended to offer practical, step-by-step instructions to solve common problems when doing data analysis for Personal Science.\n\n1 Prerequisites\nWe’ll assume some basic tools.\n\nA spreadsheet like Microsoft Excel\nThe programming language R and the associated development environment RStudio\n\nA good introduction to R is Hands On Programming with R"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "In other words, like a hungry person in a kitchen full of ingredients, you need a cookbook of recipes that can explain in a step-by-step, repeatable manner, how to go from the raw data around you to some fully-baked insights. That’s the purpose of the Personal Science Cookbook. Each “recipe” is short and self-contained. Some are more complex than others, but none require any tools or knowledge beyond what is explained in the book."
  },
  {
    "objectID": "personal_science.html",
    "href": "personal_science.html",
    "title": "3  The Principles of Personal Science",
    "section": "",
    "text": "When the first microchips enabled desktop computers in the 1970s, people were unsure what to call them.\nThe word “mini-computer” was already taken, referring to a generation of computers that didn’t require entire rooms, so the techie engineers who confronted these new machines called them “microcomputers”, a moniker that lives on in the name for one of the first software companies of that generation, Microsoft.\nSome people called them “hobby computers”, because that seemed to be all they were good for. The most influential early gathering of people using them was called the “Homebrew Computer Club”. The term “desktop” was gaining traction, and inspired later generations that called them “laptops”, but then the most traditional of all computer companies introduced its first “IBM PC”, and suddenly the industry had a new term.\nIt was a “personal computer” because, for the first time, it was cheap enough and easy enough for a single individual to use it by him (or her) self. In contrast to all previous generations of computing, everything about the device was intended to be used by a single individual. Even if the computer was shared, only one person would use it at a time, and all design decisions reflected that: a single keyboard, monitor, one power switch. You didn’t need a team of people to set up and care for the device — it was out-of-the-box something that a single person could set up and use.\nIt’s easy to forget how transformative this was at the time. Computers until then were very expensive — many times more than the cost of a car or even a house. You had to be a large organization — a university, a business — to afford one, and even if somebody magically just gave one to you, you’d need a special place to keep it, with highly-trained technicians just to keep it running, and of course even more well-trained engineers and scientists to get it to do anything useful.\n\nA similar situation exists today in science. New discoveries are made in large institutions, by teams of high-trained people with access to large, expensive equipment. The discoveries are discussed and shared by specialists who are followed by a cadre of specialized interpreters — journalists, educators, clinicians —  who decipher the new scientific results into lay language and ultimately into face-to-face interaction with the public. Committees meet to discuss takeaways from the expensive and time-consuming research, reaching conclusions that are considered generally acceptable enough to result in new actionable treatments and suggestions for “normal people”.\nThis gap between the specialists and the general public, like the gap between mainframe computers and PCs, is eroding thanks to technology.\nActually that’s not quite true: the potential gap between specialists and the general public is eroding. But reality is still different. It’s as if PCs had been invented but no software.\nThe personal computer revolution was about more than simply cheaper devices. The hardware became useful after it spawned an entire industry of dedicated software makers,  educational experts, consultants and systems integrators,\nProfessional science\nWe all think science is great…\nbut what do people mean when they say “science”? 1. Wonder  (photos of stars, micrographs, etc.) 2. Technology (photos of roman arch, integrated circuit, moon landing) 3. A way of thinking (photos of “amateur” scientists)\nIt’s tempting to assume that the scientific way of thinking is obvious, and maybe even obviously the only way to think rigorously but that’s not really true.\nAlternatives to the scientific way of thinking: recipes\nMy definition of science: a predisposition to the assumption that you’re wrong, a nasty mischievous inclination to disbelieve things you can’t prove.\nA core scientific skill is curiosity.  Always ask “what if…”  thinking in hypotheticals\nReligion seems like a classic example of unscientific thinking, but even that I’ll challenge. What if you’re wrong?  Is there a way to experiment, test it?\nScience is:\n\nCuriosity\nSkepticism : an unending belief that you are wrong\n\nLow interest in credentials … just because you are “certified” doesn’t mean you know any more than I do.\n\nBias toward experiments\n\nSee Roberts (2004) for examples.\n\n\n\n\nRoberts, Seth. 2004. “Self-Experimentation as a Source of New Ideas: Ten Examples about Sleep, Mood, Health, and Weight.” http://www.escholarship.org/uc/item/2xc2h866."
  },
  {
    "objectID": "get-started.html",
    "href": "get-started.html",
    "title": "4  Getting Started",
    "section": "",
    "text": "Let’s say you are suffering from unexplained headaches that appear somewhat randomly. You suspect they may be associated with something you eat, but you’re not sure, so you’ve been tracking 14 weeks (98 days) worth of your own data in a spreadsheet that looks like this:\n\n\nCode\nlibrary(tidyverse, quietly=TRUE)\nlibrary(lubridate, quietly=TRUE)\n\n\n\n\nCode\nset.seed(1984)\n\nx <- tibble(date=seq(from = today()-weeks(14),\n                     by = \"1 day\", length.out = 7*14),\n            headache = sample(c(TRUE,FALSE), 7*14,\n                              prob = c(.05,.95),\n                              replace = TRUE))\n\nknitr::kable( head(x) ) %>% kableExtra::kable_styling()\n\n\n\n\n \n  \n    date \n    headache \n  \n \n\n  \n    2022-06-22 \n    FALSE \n  \n  \n    2022-06-23 \n    FALSE \n  \n  \n    2022-06-24 \n    FALSE \n  \n  \n    2022-06-25 \n    FALSE \n  \n  \n    2022-06-26 \n    FALSE \n  \n  \n    2022-06-27 \n    FALSE \n  \n\n\n\n\n\nCode\nwrite_csv(x,\"headache-days.csv\")\n\n\nYou can download a copy of this file here\nIt’s easy to add a few more variables (columns) to the dataframe: (download)\n\n\nCode\nz <- function(x){\nm = NULL\nfor(i in 1:14){\n  m = c(c(rep(0,6),\n              floor(runif(1,min=0,max=3))),\n        m)\n}\n\nm\n}\nx <- tibble(date=seq(from = today()-weeks(14),\n                     by = \"1 day\", length.out = 7*14),\n            headache = sample(c(TRUE,FALSE), 7*14,\n                              prob = c(.05,.95),\n                              replace = TRUE),\n            icecream = sample(c(TRUE,FALSE), 7*14,\n                              prob = c(.10,.90),\n                              replace = TRUE),\n            z = runif(7*14, min = -2.5, max = .5) + 8,\n            wine = z(0))\n\nknitr::kable( head(x,10), digits = 2) %>%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n \n  \n    date \n    headache \n    icecream \n    z \n    wine \n  \n \n\n  \n    2022-06-22 \n    FALSE \n    TRUE \n    7.56 \n    0 \n  \n  \n    2022-06-23 \n    FALSE \n    FALSE \n    7.38 \n    0 \n  \n  \n    2022-06-24 \n    FALSE \n    FALSE \n    5.51 \n    0 \n  \n  \n    2022-06-25 \n    FALSE \n    TRUE \n    7.60 \n    0 \n  \n  \n    2022-06-26 \n    FALSE \n    FALSE \n    8.36 \n    0 \n  \n  \n    2022-06-27 \n    FALSE \n    FALSE \n    6.92 \n    0 \n  \n  \n    2022-06-28 \n    FALSE \n    FALSE \n    6.32 \n    0 \n  \n  \n    2022-06-29 \n    FALSE \n    FALSE \n    7.52 \n    0 \n  \n  \n    2022-06-30 \n    FALSE \n    FALSE \n    7.95 \n    0 \n  \n  \n    2022-07-01 \n    FALSE \n    FALSE \n    6.99 \n    0 \n  \n\n\n\n\n\nCode\nwrite_csv(x,\"headache-variables.csv\")\n\n\n\nheadache: a day when I have a headache\nicecream: did I eat ice cream that day?\nwine: Number of glasses of wine I drank.\nz: Number of hours I slept that day."
  },
  {
    "objectID": "method.html",
    "href": "method.html",
    "title": "5  Methods",
    "section": "",
    "text": "Before we discuss techniques for how to analyze your data, let’s cover a few basic methods that will be useful for all of the example solutions in this book."
  },
  {
    "objectID": "method.html#what-is-a-dataframe",
    "href": "method.html#what-is-a-dataframe",
    "title": "5  Methods",
    "section": "5.1 What is a dataframe?",
    "text": "5.1 What is a dataframe?\nSelf-collected data is almost always best represented by a table of the variables you want to study and the values that you collected for each of those variables. The most common type of table is a spreadsheet, which in Personal Science we refer to as a data table or a data frame. Abbreviated “dataframe” or often just “df”, it’s a table of values and variables that always has the same form:\n\ncolumns are variables: the parameters you want to study\nrows are observations: each incident of data you collected.\n\nIt’s important to get in the habit of this row/column approach to data collection because, as you’ll see, all of our tools assume that data will come in a data frame format.\n\n5.1.1 How do I read a dataframe?\nAlthough you are probably used to handling data frames in a spreadsheet program like Excel, in this cookbook we’ll need to start by reading the data into R.\nSolution\nUse the Tidyverse readr package. Read a CSV-formatted file with the read_csv function. Other Tidyverse let you read many other types of data, including Microsoft Excel (XLSX) files with the function readxl::read_excel().\nRegardless of where you get the data, you’ll want to read it into a dataframe. In this case, we’ll save the CSV contents into the dataframe variable headache_df.\n\n\nCode\nlibrary(tidyverse)\nheadache_df <- readr::read_csv(\"headache-variables.csv\")\nheadache_df %>% head() %>% knitr::kable()\n\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\n\n\n\n\n2022-06-22\nFALSE\nTRUE\n7.557071\n0\n\n\n2022-06-23\nFALSE\nFALSE\n7.379434\n0\n\n\n2022-06-24\nFALSE\nFALSE\n5.512368\n0\n\n\n2022-06-25\nFALSE\nTRUE\n7.600135\n0\n\n\n2022-06-26\nFALSE\nFALSE\n8.362155\n0\n\n\n2022-06-27\nFALSE\nFALSE\n6.924651\n0\n\n\n\n\n\nHere we peeked at the first 6 lines using the function head() and then sent it to the knitr::kable() function to be printed in this nice format."
  },
  {
    "objectID": "method.html#rolling-average",
    "href": "method.html#rolling-average",
    "title": "5  Methods",
    "section": "5.2 Rolling average",
    "text": "5.2 Rolling average\nA long series of daily numbers becomes unwieldy after a while, so we’d like to summarize them somehow, perhaps as groups of weeks or months.\nProblem You want to take the rolling 7-day average of a series of numbers.\nSolution use the rolling() functions in package zoo:\n\n\nCode\nlibrary(zoo)\n\nheadache_df %>% \n    mutate(sleep7A = rollapply(z,\n                               7, \n                               function(x) {x = mean(x,na.rm = TRUE)},\n                               align = 'right',\n                               fill = NA)) %>% \n  tail() %>% knitr::kable()\n\n\n\n\n\ndate\nheadache\nicecream\nz\nwine\nsleep7A\n\n\n\n\n2022-09-22\nFALSE\nFALSE\n8.120216\n0\n6.899738\n\n\n2022-09-23\nFALSE\nFALSE\n5.668099\n0\n6.738935\n\n\n2022-09-24\nFALSE\nFALSE\n8.093531\n0\n7.094138\n\n\n2022-09-25\nTRUE\nFALSE\n7.679254\n0\n7.308210\n\n\n2022-09-26\nFALSE\nFALSE\n7.346326\n0\n7.283955\n\n\n2022-09-27\nFALSE\nFALSE\n6.019567\n2\n7.321820\n\n\n\n\n\nUsing the Tidyverse mutate() function, we created a new variable sleep7A to hold the 7-day rolling average for our sleep (Z) variable.\nProblem How do we skip the days in between and summarize just the averages by week?\nSolution Use summarize().\nThe Tidyverse function lubridate::week() returns the number of complete seven day periods that have occurred between the date and January 1st, plus one.\n\n\nCode\nheadache_weeks <- headache_df %>%\n  mutate(week = lubridate::week(date)) %>%\n  group_by(week) %>%\n  summarize(week_ave = mean(z))\n\nheadache_weeks\n\n\n\n\n  \n\n\n\nCode\nheadache_weeks %>% ggplot(aes(x=week, y = week_ave)) +\n  geom_line() +\n  labs(title = \"Weekly Average for Z\", y = \"Hours (Weekly Ave)\")"
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "6  Applications",
    "section": "",
    "text": "Load the data we created in our Chapter 4 example.\nWith my 14 weeks of data, we can do a few basic calculations:\nHow frequent are my headaches? Simply total the number of headaches and divide by number of days:"
  },
  {
    "objectID": "applications.html#hypothesis",
    "href": "applications.html#hypothesis",
    "title": "6  Applications",
    "section": "6.1 Hypothesis",
    "text": "6.1 Hypothesis\nWith the data collected and in a nice dataframe format, we can start to ask what might be driving the headaches. One of the first suspected culprits might be something that I eat.\nBased on the data collected so far, can I make any guesses about what might be driving my headaches?\nThe most obvious place to check is whether I see any patterns on the days when I have headaches. Let’s filter for headache days only:\n\n\nCode\nx %>% filter(headache)  %>% kableExtra::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n \n  \n    date \n    headache \n    icecream \n    z \n    wine \n  \n \n\n  \n    2022-07-03 \n    TRUE \n    TRUE \n    8.478644 \n    0 \n  \n  \n    2022-07-18 \n    TRUE \n    FALSE \n    6.880779 \n    0 \n  \n  \n    2022-07-25 \n    TRUE \n    FALSE \n    6.909671 \n    0 \n  \n  \n    2022-08-12 \n    TRUE \n    FALSE \n    7.278277 \n    0 \n  \n  \n    2022-08-20 \n    TRUE \n    FALSE \n    7.797994 \n    0 \n  \n  \n    2022-09-05 \n    TRUE \n    TRUE \n    8.385846 \n    0 \n  \n  \n    2022-09-08 \n    TRUE \n    FALSE \n    5.849701 \n    0 \n  \n  \n    2022-09-25 \n    TRUE \n    FALSE \n    7.679254 \n    0 \n  \n\n\n\n\n\nBut maybe the headache takes a day or two to kick in. We can divide the data by week and see if we can spot any patterns in headache frequency:\n\n\nCode\nx %>% group_by(week = ntile(date,7)) %>% \n  summarise(headaches = sum(headache),\n            alcohol = sum(wine),\n            icecream = sum(icecream))  %>% kableExtra::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n \n  \n    week \n    headaches \n    alcohol \n    icecream \n  \n \n\n  \n    1 \n    1 \n    0 \n    4 \n  \n  \n    2 \n    1 \n    1 \n    0 \n  \n  \n    3 \n    1 \n    3 \n    0 \n  \n  \n    4 \n    1 \n    2 \n    1 \n  \n  \n    5 \n    1 \n    4 \n    1 \n  \n  \n    6 \n    2 \n    3 \n    3 \n  \n  \n    7 \n    1 \n    3 \n    0 \n  \n\n\n\n\n\nBuy simply eye-balling the data this way, you might see a pattern. For example, you might spot a week or two with an unusually large number of headaches and notice those weeks are accompanied by an unusually large consumption of some particular food.\nBut how do you know you’re not just guessing? What looks like a pattern might be a coincidence. To find out with more certainty, we will apply some statistics."
  },
  {
    "objectID": "applications.html#t-testing",
    "href": "applications.html#t-testing",
    "title": "6  Applications",
    "section": "6.2 T-Testing",
    "text": "6.2 T-Testing\nHint: an Excel version of this exercise is in Section 10.1 .\nThe simplest test is called a “T Test”. This is a formula that can compare two equal-sized lists of numbers and return the probability that any differences between the two are the result of chance.\nWhat are the chances that the number of headaches per week is related to the amount of ice cream I eat per week?\nIf there were a relationship between ice cream and headaches each week, I’d expect that over the weeks in this period, the total number of headaches and the total number of ice cream days should be roughly equal.\n\n\nCode\nx_week <- x %>% group_by(week = ntile(date,7)) %>% \n  summarise(headaches = sum(headache),\n            alcohol = sum(wine),\n            icecream = sum(icecream)) \nx_week %>% kableExtra::kable() %>%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) \n\n\n\n\n \n  \n    week \n    headaches \n    alcohol \n    icecream \n  \n \n\n  \n    1 \n    1 \n    0 \n    4 \n  \n  \n    2 \n    1 \n    1 \n    0 \n  \n  \n    3 \n    1 \n    3 \n    0 \n  \n  \n    4 \n    1 \n    2 \n    1 \n  \n  \n    5 \n    1 \n    4 \n    1 \n  \n  \n    6 \n    2 \n    3 \n    3 \n  \n  \n    7 \n    1 \n    3 \n    0 \n  \n\n\n\n\n\nCode\nwith(x_week, t.test(headaches,icecream))[[\"p.value\"]]\n\n\n[1] 0.8254265\n\n\nBy convention, a p-value less than 0.05 (that is, less than 5\\%) is considered statistically significant. While this is not a hard and fast rule, it’s often a good place to start. A p-value greater than this is almost certainly due to chance."
  },
  {
    "objectID": "applications.html#data-visualization",
    "href": "applications.html#data-visualization",
    "title": "6  Applications",
    "section": "6.3 Data visualization",
    "text": "6.3 Data visualization\nThe first step in a more sophisticated analysis is to plot the data to see if we can spot any particular patterns.\n\n\nCode\nx_week %>% pivot_longer(names_to = \"activity\",\n                        values_to = \"amount\",\n                        cols = alcohol:icecream ) %>% \n  ggplot(aes(x=week,y=headaches)) +\n  geom_bar(aes(x=week,y=amount, fill = activity),\n           position = \"dodge\",\n           stat = \"identity\") +\n  geom_line(aes(x=week,y=headaches))"
  },
  {
    "objectID": "microbiome.html",
    "href": "microbiome.html",
    "title": "7  Handling microbiome data",
    "section": "",
    "text": "When a microbiome sample is sequenced by a genetic sequencing machine, the results are presented in large files, called FASTQ, made of the A, C, T, G letters of the genetic code along with other information about measurement accuracy and more. The final report sent to you as a customer, builds from these files using a bioinformatics “pipeline” designed to summarize the genetic code into a more readable format. Embedded within the pipeline are dozens of assumptions about how to best interpret the genetic letters, including how to handle cases where the interpretation is unclear, or even arbitrary. For example, although the sequence of a common microbe like Streptococcus mutans is well-understood, how close does a sequence have to be before the report can confidently describe it as a member of that species? Different pipelines make different assumptions. One might say it can be off by 10 letters, while another might say 5; other pipelines might judge based on the particular microbe. And what should the report do when the sequencer returns less-than-confident results? No sequencer can be perfect all of the time, so by necessity some allowance must be made for how much leeway should be allowed in an interpretation."
  },
  {
    "objectID": "microbiome.html#the-sum-to-1-problem",
    "href": "microbiome.html#the-sum-to-1-problem",
    "title": "7  Handling microbiome data",
    "section": "7.2 The ‘sum to 1’ problem",
    "text": "7.2 The ‘sum to 1’ problem\nOnce the pipeline has been tweaked to give consistent answers for a particular lab, another question awaits.\nSince most tests report the relative abundance of a particular microbe, the totals will always sum to 100%. While this makes sense when you want to know the overall composition of the microbiome, it may not be as useful when studying how the results from one day compares to another.\nThe reason is called the “sum to 1” problem. To explain this, let’s use a concrete example."
  },
  {
    "objectID": "microbiome.html#example",
    "href": "microbiome.html#example",
    "title": "7  Handling microbiome data",
    "section": "7.3 Example",
    "text": "7.3 Example\nSuppose we have the following result for our first test:\n\n\n\nTest 1\n\n\n\n\n\n\nMicrobe\nAbsolute\nRelative\n\n\nA\n100\n10%\n\n\nB\n500\n50%\n\n\nC\n400\n40%\n\n\nD\n0\n0%\n\n\n\n\n\n\n\nTotal\n1000\n100%\n\n\n\nWe don’t specify the units in the “Absolute” column, but it can be whatever you like: grams, tons, mg/mL – it doesn’t matter. In this simple example, we measure a total of 1000 (of something) and compute the various relative amounts. All is well.\nIn our second test, for whatever reason, we collect a lot more stuff, leading to a larger absolute amount but the relative amounts are unchanged.\n\n\n\nTest 2\n\n\n\n\n\n\nMicrobe\nAbsolute\nRelative\n\n\nA\n150\n10%\n\n\nB\n750\n50%\n\n\nC\n600\n40%\n\n\nD\n0\n0%\n\n\n\n\n\n\n\nTotal\n1500\n100%\n\n\n\nBut now consider a different case. This time, for some reason one of the three microbes has a massive increase in absolute terms. Importantly, none of the other microbes changed. This might happen if your sample were somehow contaminated, for example, perhaps from some extraneous microbe entering the tube after you sampled it. Or it could be that the sampling site suddenly had a new growth of an new microbe that doesn’t affect anything else. Lots of reasons could explain why the absolute values of various microbes could be unchanged even the relative values are substantially different.\n\n\n\nTest 3A\n\n\n\n\n\n\nMicrobe\nAbsolute\nRelative\n\n\nA\n150\n8%\n\n\nB\n750\n38%\n\n\nC\n600\n30%\n\n\nD\n500\n25%\n\n\n\n\n\n\n\nTotal\n2000\n100%\n\n\n\nBut you don’t need contamination for a slight change in one microbe to have a major impact on the relative abundance of the others.\nWatch what happens when two microbes, A and B, are unchanged while two others swap abundance amounts.\n\n\n\nTest 3B\n\n\n\n\n\n\nMicrobe\nAbsolute\nRelative\n\n\nA\n150\n8%\n\n\nB\n750\n38%\n\n\nC\n700\n35%\n\n\nD\n400\n20%\n\n\n\n\n\n\n\nTotal\n2000\n100%\n\n\n\nA and B appear to have the same relative abundances they did in Test 2. This simple case matches our intuition: we expect that the relative values of A and B would be no different than Test 3A. The absolute totals are the same, so again all is well.\nBut microbes exist in an ecology. They’re not independent of one another. Often an increase or decrease in one will drive a corresponding change in another.\nConsider the interesting case where one one microbe (A) doubles in abundance, causing another (B) to halve. Although the changes are directly related to one another, it’s hard to see that in the type of relative summary we get from our report.\n\n\n\nTest 2B\n\n\n\n\n\n\nMicrobe\nAbsolute\nRelative\n\n\nA\n200\n24%\n\n\nB\n250\n29%\n\n\nC\n400\n47%\n\n\nD\n0\n0%\n\n\n\n\n\n\n\nTotal\n850\n100%\n\n\n\nIn 2B, a major change happened – the abundance of one microbe (A) exploded and caused another (B) to plunge. Although another, independent microbe (C) was completely unaffected by this change, when we look only at the relative differences, we might be fooled into thinking that C changed as well, though it didn’t.\nWhich matters more, absolute values or relative ones? To the extent that the microbiome is synthesizing or digesting various metabolites in the body, it’s clear that absolute values are what we want to watch. But absolute abundances are too hard to track – you’d need to grab the entire microbiome somehow. So instead we assume that the microbiome as a whole maintains a roughly constant absolute volume and that the only change is the relative abundances.\nIs that true? It seems unlikely. Other living populations rise and fall depending on all sorts of factors. Your backyard garden, for example, doesn’t have the same absolute volume from one day to another. If you only knew the relative percentage of tomatoes versus cucumbers, would you really know much about your harvest?"
  },
  {
    "objectID": "microbiome.html#bottom-line",
    "href": "microbiome.html#bottom-line",
    "title": "7  Handling microbiome data",
    "section": "7.4 Bottom line",
    "text": "7.4 Bottom line\nIt’s very hard to make judgements one way or another from simple comparisons of relative abundance changes from one sample to another. Too many factors determine the measured levels of the various microbes.\nDespite this, we know empirically that the overall relative abundances are reasonably stable from one collection to another. Not precisely stable, but at least at the highest, say, phylum levels, the abundances track fairly consistently from day to day. In the oral microbiome, for example, Streptococcus is almost always the lead phylum, with Neissaria and Rothia competing with a few others for second or third place.\nMeanwhile, in larger population studies of say thousands of people sampled multiple times, some significant patterns emerge of microbes that are consistently over- or under-represented in various disease states.\nOr consider our garden analogy. Knowing the relative percentage of tomatoes and cucumbers might be useful if we had data meticulously collected from thousands of other backyard gardens, along with some “metadata” about each gardener’s assessment of their harvest. You might notice, then, that gardeners unhappy with their tomato crop tend to have lower cucumber yield too. Or there might be a strong correlation between tomato yield and herbicide usage – on average. Still, many or perhaps even most gardens will be significantly different. For example, if the relative abundances appear to match the average, you might be fooled into thinking that a garden suffering from an overall poor harvest is fine.\nIn other words, treat numbers like “relative abundance” with an appropriate level of skepticism."
  },
  {
    "objectID": "playN-of-1.html",
    "href": "playN-of-1.html",
    "title": "8  Averages vs n-of-1",
    "section": "",
    "text": "Consider the following case:\n1000 people took Vitamin D for 6 months. We measured their Vitamin D levels before and after, and sure enough: the average levels after are higher than before.\nMore specifically, let’s say the average at the beginning of the study is 30 mg/nL, widely considered the absolute minimum for a healthy person.\n\n\n\n\n\nCode\nstudy <- tibble(subject=1:N,\n                vitamind=rnorm(n=N,\n                               mean=MEAN_VITAMIN_D,\n                               sd=MEAN_VITAMIN_D\n                )\n                ) %>% \n  transmute(subject, vitamind=if_else(vitamind<0, 0,vitamind))\n\nstudy_plot <- study %>% ggplot(aes(x=subject,y=vitamind)) + \n  geom_point() +\n  geom_smooth(method= lm, formula= y ~ x, color=\"red\") + \n  labs(x=\"Subject\", y = \"Vitamin D (ng/mL)\", title = \"Vitamin D levels in all subjects\")\n\nstudy_plot\n\n\n\n\n\nRed line indicates the average slope of the points\n\n\n\n\nNote that, although the average level is about 30 mg/mL, there are many subjects whose levels are considerable above and below that.\n\n\nCode\nmaxd <- max(study$vitamind)\nstudy_plot + \n  geom_rect(xmin=0,ymin=0,xmax=N, ymax=MEAN_VITAMIN_D - sd(1:MEAN_VITAMIN_D),\n            fill = \"lightblue\",\n            alpha = 0.007) +\n    geom_rect(xmin=0,ymin=MEAN_VITAMIN_D + sd(1:MEAN_VITAMIN_D),xmax=N, ymax=maxd,\n            fill = \"lightblue\",\n            alpha = 0.007) \n\n\n\n\n\nShaded area represents points more than a standard deviation outlier.\n\n\n\n\nAnother way to represent the study is with a boxplot variation called a violin plot.\n\n\nCode\nstudy %>% ggplot() + \n  geom_violin(aes(x=subject,y=vitamind),\n              draw_quantiles = c(0.25, 0.5, 0.75)) + \n  labs(x=\"\", y = \"Vitamin D (ng/mL)\", title = \"Vitamin D levels in all subjects\")\n\n\n\n\n\nThe width of this plot shows the proportion of subjects at particular Vitamin D levels. (The horizontal lines indicate the different quartiles of the data)\n\n\n\n\nTo make this more fun, let’s assume we have additional information about the subjects in our trial.\n\n\nCode\nstudy <- cbind(study,\n nationality = replicate(N, sample(c(\"USA\",\"Japan\",\"Europe\"),size=1)),\n weight = rnorm(n=N, mean = 120, sd = 50))"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Final Words",
    "section": "",
    "text": "Download a copy in PDF or ePub."
  },
  {
    "objectID": "Excelttest.html",
    "href": "Excelttest.html",
    "title": "10  Appendix: Excel Examples",
    "section": "",
    "text": "Problem\nYou tried an intervention and want to see if it worked. How likely is it that the results were chance?\nSolution\nOne of the simplest tests is a “T-Test”, sometimes called a “Student T Test”.\nStatisticians use the concept of P Value to discuss the how often a result might appear to be significant even when it’s not. While this crude measure doesn’t describe all the ways something might happen due to chance, generally the lower the P Value, the better. Professional scientists, especially those who understand statistics, will get touchy if you claim a result based purely on P Values, but for Personal Science purposes, it’s a good start. There is no “correct” cutoff value that can determine the likelihood that something is due to chance alone, but traditionally people assume that anything under 0.05 deserves a closer look.\nHere’s an example for how to do this in Excel.\nSuppose you’d like to know if taking a melotonin supplement will help you sleep longer. You’ve measured your daily sleep, taking the supplements on some days (the “intervention”) and not on others (“control”).\nA simple spreadsheet might look like this:\n\nTrack your sleep under two columns: one for nights when you took the supplement, and the other for nights you didn’t.\nThe built-in Excel statistical function T.TEST will calculate the P-Value when you give it two ranges, the “intervention” (nights we took melotonin) and the “control” (nights without).\nSee the screenshot for the exact formula in this case:\n=T.TEST(array1,array2,tails,type)\nEnter a 1 for tails (because we’re only interested in one measurement, sleep) and a 2 for type (because in this case our samples are not of the same length).\nThe P Value in this example, 0.24, is above 0.05 and therefore we will assume that any difference in sleep between the nights is due to pure chance."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "11  References",
    "section": "",
    "text": "Roberts, Seth. 2004. “Self-Experimentation as a Source of New\nIdeas: Ten Examples about Sleep, Mood, Health, and\nWeight.” http://www.escholarship.org/uc/item/2xc2h866."
  }
]